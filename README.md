# Amazon ReviewsÂ Sentiment PipelineÂ ğŸš€

A lightweight twoâ€‘notebook workflow that lets you

1. build strong TFâ€‘IDF + classical baselines,  
2. fineâ€‘tune DistilBERT,  
3. compare the two approaches on identical splits.

Everything runs on free GoogleÂ Colab GPUs.

---

## ğŸ“¦ What Youâ€™ll Need

| Requirement | Why itâ€™s needed |Â Links |
|-------------|----------------|-------|
| **GoogleÂ Colab** with a GPU runtime (`RuntimeÂ â†’Â ChangeÂ runtimeÂ type â†’Â GPU`) | Training DistilBERT quickly | â€”
| **Kaggle account** + `kaggle.json` credentials | Automatic download of the AmazonÂ Reviews dataset | [Docs](https://www.kaggle.com/docs/api) |
| **~5Â GB of free Drive / local disk** | Caches datasets, checkpoints, and embeddings | â€”

---

## âš¡ Quickâ€‘Start (4Â Moves)

| # | Do thisÂ â†’ | ProducesÂ â†“ |
|---|-----------|-----------|
| **1** | Open **`Part_1_baseline_kaggle.ipynb`** and run every cell **up to** the heading **â€œCLSÂ embeddings for baselineÂ modelsâ€**. | â€” |
| **2** | Zip **`amazon_reviews_data/`** (it now contains `arrow_clean/` with `trainÂ /Â validationÂ /Â test` splits) and download it. | `amazon_reviews_data.zip` |
| **3** | Run **`Part_2_DistilBert.ipynb`** topâ€‘toâ€‘bottom. | `cls_embeddings.zip` <br>*(only download the embeddings of the **winning** DistilBERT model)* |
| **4** | Go back to **`PartÂ 1`**, upload the contents of `cls_embeddings.zip` into `cls_embeddings/`, then execute the remainder of that notebook. | Final baselineÂ vsÂ DistilBERT scores |

> **Tip:** StepsÂ 2 &Â 3 each show a *single* zipâ€‘file link in the Colab sidebar; just click to download.

---

## ğŸ—‚ï¸ Directory Cheatâ€‘Sheet

```
amazon_reviews_data/
 â”œâ”€ arrow_clean/              â† HFÂ DatasetDict (train / val / test)
 â””â”€ artifacts/                â† TFâ€‘IDF vectoriser & best classical model
cls_embeddings/
 â”œâ”€ <exp>_train_embeddings.npy
 â”œâ”€ <exp>_validation_embeddings.npy
 â””â”€ <exp>_test_embeddings.npy  â† upload these before rerunning PartÂ 1 tail
```

---

## â“Â FAQ / Troubleshooting

* **Notebooks look blank on GitHub.**  
  GitHub often fails to render large Colab notebooks. Click **â€œOpen inÂ Colabâ€** and they load fine. (GitHub issueÂ #155944)

* **â€œkaggle: commandÂ notÂ foundâ€**  
  Run `!pip install kaggle` in the first cell of *PartÂ 1* and reâ€‘upload `kaggle.json`.

* **CUDA outâ€‘ofâ€‘memory during DistilBERT fineâ€‘tune**  
  Switch Colab to a fresh session or reduce `batch_size` in the configuration cell near the top of *PartÂ 2*.

---

## ğŸ“„ Project Files

| Notebook / Script | Purpose |
|-------------------|---------|
| **`Part_1_baseline_kaggle.ipynb`** | Download & clean data, create classical baselines, evaluate CLSÂ embeddings |
| **`Part_2_DistilBert.ipynb`** | Comprehensive DistilBERT fineâ€‘tuning, hyperâ€‘parameter search, CLS extraction |
| `Part1-Part3-LogRegSVM-CLS_embeddings.py` | Python export of *PartÂ 1* logic (optional CLI use) |
| `Part2_DistilBert.py` | Python export of *PartÂ 2* (optional CLI use) |

---



